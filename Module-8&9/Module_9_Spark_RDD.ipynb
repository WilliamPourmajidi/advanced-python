{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb10d3ad",
   "metadata": {},
   "source": [
    "# Module 9: Working with RDDs and Shared Variables in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309ea8c",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will cover three major topics from Module 9:\n",
    "\n",
    "1. **Load and Save Data Using RDD**\n",
    "2. **Analyzing Hadoop Data with RDD**\n",
    "3. **Using Broadcast and Accumulator Variables**\n",
    "\n",
    "Each section includes explanations, example code, and interpretation of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8ff3f",
   "metadata": {},
   "source": [
    "## 1. Loading and Saving Data Using RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e4c6a",
   "metadata": {},
   "source": [
    "\n",
    "RDDs (Resilient Distributed Datasets) are Spark's original distributed collection abstraction.\n",
    "\n",
    "We can create RDDs from external data (e.g., `.txt` files) or from existing Python collections. Below is an example that loads a dataset from a file and saves it back to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5baaa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Advanced Python', 'Pourmajidi', 'William', 'Apache Spark']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Module9-RDD\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create RDD from a local list\n",
    "data = [\"William\", \"Pourmajidi\", \"Advanced Python\", \"Apache Spark\"]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Delete existing directory\n",
    "output_dir = \"output_rdd\"\n",
    "if output_dir in sc._jvm.java.lang.System.getProperty(\"user.dir\"):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "# Save RDD to text file (this creates a folder with part-0000 files)\n",
    "rdd.saveAsTextFile(\"output_rdd\")\n",
    "\n",
    "# Load RDD back from file\n",
    "loaded_rdd = sc.textFile(\"output_rdd\")\n",
    "loaded_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681df7b4",
   "metadata": {},
   "source": [
    "\n",
    "This demonstrates how RDDs can persist and be reloaded from storage, making them suitable for batch-processing pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd02469",
   "metadata": {},
   "source": [
    "## 2. Analyzing Simulated Hadoop Data Using RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e4937",
   "metadata": {},
   "source": [
    "\n",
    "We'll simulate Hadoop-style logs with synthetic text and show how RDDs can help in analysis tasks like counting and filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8986e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('WARN', 1), ('ERROR', 2), ('INFO', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulated Hadoop logs\n",
    "log_data = [\n",
    "    \"ERROR 2025-07-01 Connection refused\",\n",
    "    \"INFO 2025-07-01 Service started\",\n",
    "    \"ERROR 2025-07-02 Disk full\",\n",
    "    \"WARN 2025-07-02 High memory usage\",\n",
    "    \"INFO 2025-07-03 Job completed\"\n",
    "]\n",
    "log_rdd = sc.parallelize(log_data)\n",
    "\n",
    "# Define a function to extract log level and count\n",
    "# The function returns a tuple of (log level, count)\n",
    "def extract_log_level(line):\n",
    "    log_level = line.split()[0]\n",
    "    count = 1\n",
    "    return (log_level, count)\n",
    "\n",
    "# Define a function to reduce log level counts\n",
    "def reduce_log_level_counts(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Count log levels\n",
    "counts = log_rdd.map(extract_log_level).reduceByKey(reduce_log_level_counts)\n",
    "counts.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ba1b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**MapReduce Paradigm and Spark**\n",
    "=====================================\n",
    "\n",
    "### Overview of MapReduce\n",
    "\n",
    "The MapReduce paradigm is a programming model used for processing large datasets in a distributed computing environment. It consists of three main steps:\n",
    "\n",
    "#### Map\n",
    "\n",
    "* Take a large dataset, break it down into smaller chunks, and apply a transformation to each chunk.\n",
    "* This produces a new dataset with the transformed data.\n",
    "\n",
    "#### Shuffle\n",
    "\n",
    "* Rearrange the data to group similar keys together.\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "* Apply a reduction function to each group of data, producing a smaller output dataset.\n",
    "\n",
    "### Spark's Map and Reduce Operations\n",
    "\n",
    "In Spark, the `map` and `reduce` operations are similar to the MapReduce paradigm, but with some differences:\n",
    "\n",
    "#### Map\n",
    "\n",
    "* In Spark, the `map` operation is performed by applying a transformation function to each element of an RDD (Resilient Distributed Dataset).\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "* In Spark, the `reduce` operation is performed by applying a reduction function to each group of data, producing a smaller output dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce278152",
   "metadata": {},
   "source": [
    "\n",
    "This shows how RDD transformations and actions can efficiently process large-scale log data typical of Hadoop environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86112f3a",
   "metadata": {},
   "source": [
    "## 3. Using Broadcast and Accumulator Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e9bd4",
   "metadata": {},
   "source": [
    "\n",
    "**Broadcast** variables are read-only and shared across nodes.\n",
    "\n",
    "**Accumulators** are write-only variables used for counting operations during distributed execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb11c24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'python', 'spark', 'pyspark']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of words to be processed\n",
    "words = [\"spark\", \"python\", \"big\", \"data\", \"spark\", \"pyspark\"]\n",
    "\n",
    "# Broadcast a list of stopwords to all nodes in the cluster\n",
    "# This is useful when we need to access the same data from multiple nodes\n",
    "broadcast_stopwords = sc.broadcast([\"big\", \"data\"])\n",
    "\n",
    "# Create an RDD from the list of words\n",
    "# This will split the data into smaller chunks and distribute it across the cluster\n",
    "word_rdd = sc.parallelize(words)\n",
    "\n",
    "# Filter out the words that are in the broadcasted stopwords list\n",
    "# The lambda function is applied to each word in the RDD\n",
    "# The word is included in the filtered RDD if it is not in the stopwords list\n",
    "def filter_out_stopwords(word):\n",
    "    # Get the list of stopwords from the broadcasted object\n",
    "    stopwords = broadcast_stopwords.value\n",
    "    \n",
    "    # Check if the word is not in the list of stopwords\n",
    "    if word not in stopwords:\n",
    "        # If the word is not a stopword, return True to include it in the filtered RDD\n",
    "        return True\n",
    "    else:\n",
    "        # If the word is a stopword, return False to exclude it from the filtered RDD\n",
    "        return False\n",
    "\n",
    "# Filter out the words that are in the broadcasted stopwords list\n",
    "filtered = word_rdd.filter(filter_out_stopwords)\n",
    "\n",
    "# Collect the filtered RDD and print the results\n",
    "# This will bring the data back to the driver node and print it to the console\n",
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab9e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an accumulator to keep track of a value across multiple nodes in the cluster\n",
    "# Accumulators are useful when we need to aggregate data from multiple nodes\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "# Define a function to count the occurrences of the word \"spark\"\n",
    "# This function will be applied to each word in the RDD\n",
    "def count_spark(word):\n",
    "    # Use the global keyword to access the accumulator variable\n",
    "    # This is necessary because the accumulator is defined outside the function\n",
    "    global accum\n",
    "    \n",
    "    # Check if the word is \"spark\"\n",
    "    if word == \"spark\":\n",
    "        # If the word is \"spark\", increment the accumulator by 1\n",
    "        accum += 1\n",
    "\n",
    "# Apply the count_spark function to each word in the RDD\n",
    "# The foreach method applies a function to each element in the RDD, but does not return anything\n",
    "word_rdd.foreach(count_spark)\n",
    "\n",
    "# Get the final value of the accumulator\n",
    "# This will give us the total count of \"spark\" across all nodes in the cluster\n",
    "accum.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
